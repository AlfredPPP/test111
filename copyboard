ğŸ‘Œå¾ˆå¥½ï¼Œä½ çš„é¡¹ç›®å·²ç»æœ‰äº†æ¯”è¾ƒæ¸…æ™°çš„ DDDï¼ˆé¢†åŸŸé©±åŠ¨è®¾è®¡ï¼‰+ åˆ†å±‚ç»“æ„ï¼Œä½ ç°åœ¨éœ€è¦æŠŠã€Œçˆ¬è™«ã€è¿™ä¸€å—ç†æ¸…æ¥šã€‚
æˆ‘ç»“åˆä½ ä¼ çš„ requirements.docxï¼ˆé‡Œé¢æåˆ°çš„æ•°æ®æºã€æ›´æ–°é€»è¾‘ã€è§£æé€»è¾‘ï¼‰+ ç°æœ‰é¡¹ç›®ç»“æ„ï¼Œå¸®ä½ æ¢³ç†ä¸€ä¸ªæ¶æ„æ€è·¯ï¼š

â¸»

ğŸ— çˆ¬è™«æ¶æ„å»ºè®®

ä½ çš„é¡¹ç›®å·²ç»åŒºåˆ†äº† scrapersï¼ˆæŠ“å–æ•°æ®çš„æœ€ä½å±‚ï¼‰ å’Œ business/servicesï¼ˆä¸šåŠ¡é€»è¾‘æœåŠ¡å±‚ï¼‰ã€‚
å¯ä»¥è¿™æ ·åˆ†å·¥ï¼š

1. scrapers/ ï¼ˆæœ€ä½å±‚ï¼Œæ•°æ®æŠ“å–ï¼‰
	â€¢	æ¯ä¸ªæ–‡ä»¶å¯¹åº”ä¸€ä¸ªæ•°æ®æºï¼š
	â€¢	asx_scraper.py
	â€¢	vanguard_scraper.py
	â€¢	betashares_scraper.py
	â€¢	ishares_scraper.py

ç‰¹ç‚¹ï¼š
	â€¢	åªè´Ÿè´£ã€ŒæŠ“å– HTML/JSON/PDF æ–‡ä»¶ã€â†’ è¿”å›åŸå§‹æ•°æ®ï¼ˆdict / JSON / bytesï¼‰ã€‚
	â€¢	ä½¿ç”¨ aiohttp å®ç°å¼‚æ­¥è¯·æ±‚ã€‚
	â€¢	ä¸ç›´æ¥æ“ä½œæ•°æ®åº“ã€‚

ç¤ºä¾‹ (asx_scraper.py)ï¼š

import aiohttp
import asyncio

class AsxScraper:
    BASE_URL = "https://www.asx.com.au/asx/announcements/"

    async def fetch_announcements(self, code: str, start_date, end_date):
        params = {
            "asx_code": code,
            "from_date": start_date.strftime("%Y-%m-%d"),
            "to_date": end_date.strftime("%Y-%m-%d"),
        }
        async with aiohttp.ClientSession() as session:
            async with session.get(self.BASE_URL, params=params) as resp:
                return await resp.json()  # å¯èƒ½æ˜¯JSON / HTML


â¸»

2. business/services/spider_service.py ï¼ˆä¸šåŠ¡é€»è¾‘å±‚ï¼‰
	â€¢	è°ƒç”¨ scrapers â†’ æ‹¿åˆ°åŸå§‹æ•°æ®ã€‚
	â€¢	è°ƒç”¨ parsers â†’ æŠŠåŸå§‹æ•°æ®è§£ææˆç»“æ„åŒ–å¯¹è±¡ï¼ˆdictï¼‰ã€‚
	â€¢	è°ƒç”¨ database.repositories â†’ æŠŠç»“æœå­˜è¿›æ•°æ®åº“ã€‚
	â€¢	è°ƒç”¨ system_log_repository â†’ è®°å½•æ—¥å¿—ã€‚

å®ƒç›¸å½“äºã€Œçˆ¬è™«è°ƒåº¦å™¨ + æ•°æ®è½åº“æœåŠ¡ã€ã€‚

ç¤ºä¾‹ (spider_service.py)ï¼š

from datetime import date
from src.dmh_mr_tool.scrapers.asx_scraper import AsxScraper
from src.dmh_mr_tool.business.services.parser_service import ParserService
from src.dmh_mr_tool.database.connection import DatabaseManager
from src.dmh_mr_tool.database.repositories.asx_repository import AsxInfoRepository

class SpiderService:
    def __init__(self, db_manager: DatabaseManager):
        self.db_manager = db_manager
        self.scraper = AsxScraper()
        self.parser = ParserService()

    async def crawl_asx(self, code: str, start: date, end: date, user="system"):
        raw_data = await self.scraper.fetch_announcements(code, start, end)
        parsed = self.parser.parse_asx(raw_data)

        with self.db_manager.session() as session:
            repo = AsxInfoRepository(session)
            for item in parsed:
                if not repo.find_duplicate(item["asx_code"], item["title"], item["pub_date"]):
                    repo.create(
                        asx_code=item["asx_code"],
                        title=item["title"],
                        pub_date=item["pub_date"],
                        pdf_url=item["pdf_url"],
                        update_user=user
                    )


â¸»

3. parsers/ ï¼ˆæ•°æ®è§£æå±‚ï¼‰
	â€¢	ä¸“æ³¨äºã€ŒæŠŠ HTML / JSON / PDFã€è½¬æˆç»Ÿä¸€çš„ç»“æ„åŒ–æ ¼å¼ï¼ˆdict æˆ–æ¨¡å‹ï¼‰ã€‚
	â€¢	ä½ å·²ç»æœ‰ parser_service.pyï¼Œå®ƒå¯ä»¥åšç»Ÿä¸€è°ƒåº¦ã€‚
	â€¢	ä¾‹å¦‚ pdf_parser.pyã€excel_parser.py å°±æ˜¯ä¸“é—¨çš„å·¥å…·ç±»ã€‚

â¸»

4. main.py ï¼ˆå…¥å£è°ƒåº¦ï¼‰
	â€¢	ç”¨æˆ·ä» UI ç•Œé¢ç‚¹ã€Œå¼€å§‹çˆ¬è™«ã€ â†’ UI è°ƒç”¨ SpiderService â†’ å¼€å§‹çˆ¬å–æ•°æ®å¹¶å…¥åº“ã€‚
	â€¢	æˆ–è€…ä½ å¯ä»¥åœ¨å‘½ä»¤è¡Œé‡Œè°ƒç”¨ï¼Œæ¯”å¦‚ï¼š

import asyncio
from src.dmh_mr_tool.database.connection import DatabaseManager
from src.dmh_mr_tool.config.settings import DatabaseConfig
from src.dmh_mr_tool.business.services.spider_service import SpiderService
from datetime import date

async def main():
    db = DatabaseManager(DatabaseConfig(path="dmh.db", echo=True))
    db.initialize()
    spider = SpiderService(db)

    await spider.crawl_asx("BHP", date(2025,1,1), date(2025,8,20))

if __name__ == "__main__":
    asyncio.run(main())


â¸»

ğŸ“Œ è°ƒç”¨å…³ç³»æ€»ç»“
	1.	UI å±‚ï¼ˆui/spider_view.pyï¼‰
	â€¢	ç”¨æˆ·æ“ä½œ â†’ è°ƒç”¨ SpiderServiceã€‚
	2.	ä¸šåŠ¡å±‚ï¼ˆbusiness/services/spider_service.pyï¼‰
	â€¢	è°ƒåº¦ scrapers â†’ æ‹¿æ•°æ®ã€‚
	â€¢	è°ƒç”¨ parsers â†’ è§£æã€‚
	â€¢	è°ƒç”¨ repositories â†’ å…¥åº“ã€‚
	3.	æ•°æ®è·å–å±‚ï¼ˆscrapers/*.pyï¼‰
	â€¢	ç”¨ aiohttp è¯·æ±‚ç½‘ç«™ â†’ è¿”å›åŸå§‹æ•°æ®ã€‚
	4.	è§£æå±‚ï¼ˆparsers/*.pyï¼‰
	â€¢	è§£æ HTML / JSON / PDF â†’ æ ‡å‡†åŒ– dictã€‚
	5.	æ•°æ®åº“å±‚ï¼ˆdatabase/*.pyï¼‰
	â€¢	repositories å†™å…¥æ•°æ®åº“ã€‚
	â€¢	connection ç®¡ç†äº‹åŠ¡ã€‚

â¸»

âœ… è¿™æ ·åˆ†å±‚çš„å¥½å¤„ï¼š
	â€¢	scrapers åªç®¡ã€ŒæŠ“æ•°æ®ã€ï¼Œæ–¹ä¾¿æµ‹è¯•å’Œæ›¿æ¢ã€‚
	â€¢	parsers åªç®¡ã€Œè§£æã€ï¼Œå¯ä»¥å•ç‹¬ç»´æŠ¤è§„åˆ™ã€‚
	â€¢	spider_service ç®¡ç†ã€ŒæŠ“å– â†’ è§£æ â†’ å…¥åº“ã€çš„å…¨æµç¨‹ã€‚
	â€¢	repositories ä¿è¯æ•°æ®åº“æ“ä½œç»Ÿä¸€ã€‚

â¸»

è¦ä¸è¦æˆ‘å¸®ä½ ç”»ä¸€ä¸ª è°ƒç”¨æµç¨‹å›¾ï¼Œè®©ä½ æ›´ç›´è§‚åœ°çœ‹åˆ°ã€ŒUI â†’ spider_service â†’ scrapers + parsers â†’ databaseã€çš„å…¨é“¾è·¯ï¼Ÿ